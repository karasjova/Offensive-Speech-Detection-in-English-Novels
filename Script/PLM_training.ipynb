{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport torch\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.metrics import f1_score\nfrom datasets import Dataset\nfrom transformers import (\n    pipeline, AutoTokenizer, AutoModelForSequenceClassification,\n    Trainer, TrainingArguments, DataCollatorWithPadding\n)\n\n# ====================\n# Paths\n# ====================\nDATA_PATH = \"/kaggle/input/new-salinger-labelled/new_salinger_labelled.csv\"\nOUT_DIR = \"experiments_results\"\nos.makedirs(OUT_DIR, exist_ok=True)\n\n# ====================\n# Load dataset\n# ====================\ndf = pd.read_csv(DATA_PATH, sep=\";\")\ndf.columns = df.columns.str.strip()\nlabel_map = {'offensive': 1, 'not_offensive': 0}\ndf['label'] = df['offensive'].map(label_map)\n\n# ====================\n# Scenarios\n# ====================\ndf['sentence_only'] = df['sentence']\ndf['sentence_plus_first_word'] = df['sentence'] + \" \" + df['word']\ndf['sentence_plus_all_words'] = df.groupby('sentence')['word'].transform(lambda x: ' '.join(x))\ndf['sentence_plus_all_words'] = df['sentence'] + \" \" + df['sentence_plus_all_words']\nscenarios = ['sentence_only', 'sentence_plus_first_word', 'sentence_plus_all_words']\n\ndevice = 0 if torch.cuda.is_available() else -1\n\nplm_results = []\n\n# ====================\n# Zero-shot setting\n# ====================\n\nplm_zero_shot_models = {\n    \"ZeroShot-BART\": \"facebook/bart-large-mnli\",\n    \"ZeroShot-HateBERT\": \"GroNLP/hateBERT\",\n    \"ZeroShot-HateXplain\": \"Hate-speech-CNERG/bert-base-uncased-hatexplain\"\n}\n\nfor model_name, model_path in plm_zero_shot_models.items():\n    if model_name == \"ZeroShot-BART\":\n        classifier = pipeline(\n            \"zero-shot-classification\",\n            model=model_path,\n            device=device\n        )\n        candidate_labels = [\"offensive\", \"not_offensive\"]\n\n    else:\n        tokenizer = AutoTokenizer.from_pretrained(model_path)\n        model = AutoModelForSequenceClassification.from_pretrained(model_path)  \n        classifier = pipeline(\n            \"text-classification\",\n            model=model,\n            tokenizer=tokenizer,\n            device=device\n        )\n\n    for scenario in scenarios:\n        y_true = df['label'].tolist()\n        y_pred = []\n\n        for sent in df[scenario].tolist():\n            if model_name == \"ZeroShot-BART\":\n                res = classifier(sent, candidate_labels=candidate_labels)\n                pred = 1 if res['labels'][0] == 'offensive' else 0\n\n            elif model_name == \"ZeroShot-HateBERT\":\n                res = classifier(sent)\n                # LABEL_0 / LABEL_1 → преобразуем в 0/1\n                pred = 1 if res[0][\"label\"] == \"LABEL_1\" else 0\n\n            else:  # ZeroShot-HateXplain\n                res = classifier(sent)\n                label_upper = res[0][\"label\"].upper()\n                pred = 1 if (\"HATE\" in label_upper) or (\"OFF\" in label_upper) else 0\n\n            y_pred.append(pred)\n\n        f1 = f1_score(y_true, y_pred)\n        plm_results.append({\n            \"model\": model_name,\n            \"scenario\": scenario,\n            \"split\": \"zero-shot\",\n            \"mean_f1\": f1\n        })\n        print(f\"{model_name} | {scenario} | F1: {f1:.3f}\")\n\n# ====================\n# Fine-tuned models 80-20 split\n# ====================\nft_models = {\n    \"HateBERT\": \"GroNLP/hateBERT\",\n    \"HateXplain\": \"Hate-speech-CNERG/bert-base-uncased-hatexplain\"\n}\n\nfor model_name, model_path in ft_models.items():\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n    model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=2, ignore_mismatched_sizes=True)\n\n    for scenario in scenarios:\n        temp_df = df[[scenario, 'label']].rename(columns={scenario:'input_text'})\n        train_df, test_df = train_test_split(temp_df, test_size=0.2, stratify=temp_df['label'], random_state=42)\n\n        train_dataset = Dataset.from_pandas(train_df)\n        test_dataset = Dataset.from_pandas(test_df)\n\n        def tokenize(batch):\n            return tokenizer(batch[\"input_text\"], truncation=True, padding=True)\n        train_dataset = train_dataset.map(tokenize, batched=True)\n        test_dataset = test_dataset.map(tokenize, batched=True)\n        train_dataset = train_dataset.remove_columns([\"input_text\", \"__index_level_0__\"])\n        test_dataset = test_dataset.remove_columns([\"input_text\", \"__index_level_0__\"])\n\n        data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n        training_args = TrainingArguments(\n            output_dir=f\"./{model_name}-{scenario}-80-20\",\n            eval_strategy=\"epoch\",\n            per_device_train_batch_size=16,\n            per_device_eval_batch_size=16,\n            num_train_epochs=3,\n            logging_dir=\"./logs\",\n            logging_steps=10,\n            save_strategy=\"no\",\n            report_to=[]\n        )\n\n        trainer = Trainer(\n            model=model,\n            args=training_args,\n            train_dataset=train_dataset,\n            eval_dataset=test_dataset,\n            tokenizer=tokenizer,\n            data_collator=data_collator\n        )\n\n        trainer.train()\n        predictions = trainer.predict(test_dataset)\n        logits = predictions.predictions\n        if isinstance(logits, tuple):\n            logits = logits[0]\n        if isinstance(logits, torch.Tensor):\n            logits = logits.detach().cpu().numpy()\n        y_pred = np.argmax(logits, axis=1)\n        f1 = f1_score(predictions.label_ids, y_pred)\n\n        plm_results.append({\n            \"model\": model_name+\"-FT\",\n            \"scenario\": scenario,\n            \"split\": \"80-20\",\n            \"mean_f1\": f1\n        })\n\n# ====================\n# Fine-tuned models 5-fold CV\n# ====================\nn_splits = 5\nskf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n\nfor model_name, model_path in ft_models.items():\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n\n    for scenario in scenarios:\n        temp_df = df[[scenario, 'label']].rename(columns={scenario:'input_text'})\n        X = temp_df['input_text'].tolist()\n        y = temp_df['label'].tolist()\n        f1_scores = []\n\n        for fold_idx, (train_idx, test_idx) in enumerate(skf.split(X, y), start=1):\n            X_train = [X[i] for i in train_idx]\n            y_train = [y[i] for i in train_idx]\n            X_test = [X[i] for i in test_idx]\n            y_test = [y[i] for i in test_idx]\n\n            train_dataset = Dataset.from_pandas(pd.DataFrame({'input_text': X_train, 'label': y_train}))\n            test_dataset = Dataset.from_pandas(pd.DataFrame({'input_text': X_test, 'label': y_test}))\n\n            train_dataset = train_dataset.map(tokenize, batched=True)\n            test_dataset = test_dataset.map(tokenize, batched=True)\n            train_dataset = train_dataset.remove_columns([\"input_text\"])\n            test_dataset = test_dataset.remove_columns([\"input_text\"])\n            data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\n            training_args = TrainingArguments(\n                output_dir=f\"./{model_name}-{scenario}-fold{fold_idx}\",\n                eval_strategy=\"epoch\",\n                per_device_train_batch_size=16,\n                per_device_eval_batch_size=16,\n                num_train_epochs=3,\n                logging_dir=\"./logs\",\n                logging_steps=10,\n                save_strategy=\"no\",\n                report_to=[]\n            )\n\n            model = AutoModelForSequenceClassification.from_pretrained(\n                model_path, num_labels=2, ignore_mismatched_sizes=True\n            )\n\n            trainer = Trainer(\n                model=model,\n                args=training_args,\n                train_dataset=train_dataset,\n                eval_dataset=test_dataset,\n                tokenizer=tokenizer,\n                data_collator=data_collator\n            )\n\n            trainer.train()\n            predictions = trainer.predict(test_dataset)\n            logits = predictions.predictions\n            if isinstance(logits, tuple):\n                logits = logits[0]\n            if isinstance(logits, torch.Tensor):\n                logits = logits.detach().cpu().numpy()\n            y_pred = np.argmax(logits, axis=1)\n            f1_scores.append(f1_score(y_test, y_pred))\n\n        mean_f1 = np.mean(f1_scores)\n        plm_results.append({\n            \"model\": model_name+\"-FT\",\n            \"scenario\": scenario,\n            \"split\": \"5-fold CV\",\n            \"mean_f1\": mean_f1\n        })\n\n# ====================\n# Save results\n# ====================\ndf_results = pd.DataFrame(plm_results)\ncsv_path = os.path.join(OUT_DIR, \"plm_results_combined.csv\")\ndf_results.to_csv(csv_path, index=False)\nprint(\"Results saved to CSV:\", csv_path)","metadata":{"_uuid":"cfbf6af2-5c85-4761-9bde-2cb3cb8b9927","_cell_guid":"056c753b-612f-44a9-a616-22d88e26a6c1","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}
