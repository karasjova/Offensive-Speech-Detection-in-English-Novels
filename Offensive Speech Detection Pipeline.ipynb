{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport re\nimport pandas as pd\nimport numpy as np\nimport torch\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport nltk\nfrom nltk.sentiment import SentimentIntensityAnalyzer\nfrom textblob import TextBlob\nimport spacy\nfrom collections import Counter\nfrom nltk.stem import PorterStemmer\nfrom sentence_transformers import SentenceTransformer, util\n\nfrom transformers import (\n    pipeline, AutoTokenizer, AutoModelForSequenceClassification,\n    Trainer, TrainingArguments, DataCollatorWithPadding, AutoConfig\n)\nfrom datasets import Dataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\nfrom scipy.special import softmax\nfrom sklearn.model_selection import StratifiedKFold\n\n\ndevice = 0 if torch.cuda.is_available() else -1\nprint(\"Using device:\", \"CUDA\" if device==0 else \"CPU\")\n\nnltk.download(\"punkt\")\nnltk.download(\"vader_lexicon\")\nvader = SentimentIntensityAnalyzer()\n\n# ========================\n# Paths\n# ========================\nbook_path = \"/kaggle/input/last-exit-to-brooklyn/last-exit-to-brooklyn.txt\"\nhurtlex_path = \"/kaggle/input/hurtkex-filtered/hurtlex_filtered.xlsx\"\nlabelled_path = \"/kaggle/input/new-salinger-labelled/new_salinger_labelled.csv\"\nmatched_csv = \"matched_sentences_hatebert.csv\"\nfinetuned_model_dir = \"./hatebert-finetuned\"\nbatch_size_pred = 16\nsimilarity_threshold = 0.6\n\n# ========================\n# Step 1: Load & tokenize text\n# ========================\n\ndef preprocess_text(text: str, remove_nonprintable: bool = True) -> str:\n    text = re.sub(r'\\r?\\n+', ' ', text)\n    text = re.sub(r'\\s{2,}', ' ', text)\n    if remove_nonprintable:\n        text = ''.join(c for c in text if c.isprintable())\n    text = text.strip()\n    return text\n    \nwith open(book_path, \"r\", encoding=\"utf-8\") as f:\n    raw_text = f.read()\n\ntext = preprocess_text(raw_text)\n    \ndef tokenize_text(text: str, lower: bool=True, only_alpha: bool=True):\n    if lower:\n        text = text.lower()\n    tokens = nltk.word_tokenize(text)\n    if only_alpha:\n        tokens = [t for t in tokens if t.isalpha()]\n    freq_dict = Counter(tokens)\n    return list(freq_dict.keys()), freq_dict\n\ntokens, freq_dict = tokenize_text(text)\nprint(f\"Unique tokens count: {len(tokens)}\")\n\n# ========================\n# Step 2: Load HurtLex\n# ========================\nhurtlex_df = pd.read_excel(hurtlex_path)\nhurtlex_words = set(hurtlex_df[\"lemma\"].dropna().str.lower().unique())\nprint(f\"Unique HurtLex words count: {len(hurtlex_words)}\")\nhurtlex_list = list(hurtlex_words)\n\n# ========================\n# Step 3: Expand lexicon\n# ========================\nstemmer = PorterStemmer()\nmodel_st = SentenceTransformer('all-MiniLM-L6-v2')\n\ntext_embeddings = model_st.encode(tokens, convert_to_tensor=True)\nhurtlex_embeddings = model_st.encode(hurtlex_list, convert_to_tensor=True)\n\nresults = []\n\nfor i, token in enumerate(tokens):\n    token_emb = text_embeddings[i]\n    cosine_scores = util.cos_sim(token_emb, hurtlex_embeddings)[0]\n    max_score = float(cosine_scores.max())\n    matched_idx = int(torch.argmax(cosine_scores))\n    matched_word = hurtlex_list[matched_idx]\n\n    if max_score < similarity_threshold:\n        continue\n    if token == matched_word:\n        continue\n\n    # Polarity\n    vader_compound = vader.polarity_scores(token)[\"compound\"]\n    tb_polarity = TextBlob(token).sentiment.polarity\n    level_of_agreement = int((vader_compound < 0) and (tb_polarity < 0))\n\n    token_type = \"extra_form\" if stemmer.stem(token)==stemmer.stem(matched_word) else \"new_lemma\"\n\n    results.append({\n        \"token\": token,\n        \"matched_word\": matched_word,\n        \"similarity\": max_score,\n        \"vader_compound\": vader_compound,\n        \"textblob_polarity\": tb_polarity,\n        \"level_of_agreement\": level_of_agreement,\n        \"type\": token_type\n    })\n\ndf_results = pd.DataFrame(results)\nneg_tokens_set = set(df_results[\"token\"].unique())\ndf_neg_tokens = pd.DataFrame({\"word\": list(neg_tokens_set)})\ndf_neg_tokens.to_csv(\"neg_tokens_set.csv\", index=False, encoding=\"utf-8-sig\")\nprint(f\"Negative tokens: {len(neg_tokens_set)}\")\nall_words_set = neg_tokens_set.union(hurtlex_words)\nprint(f\"All words in expanded lexicon: {len(all_words_set)}\")\n\n# ========================\n# Step 4: Preprocess text into sentences\n# ========================\nnlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"parser\"])\nif \"sentencizer\" not in nlp.pipe_names:\n    nlp.add_pipe(\"sentencizer\")\n\ndoc = nlp(text)\nsentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n\n# ========================\n# Step 5: Filter sentences by expanded lexicon\n# ========================\nmatched_sentences = []\nfor sent in sentences:\n    sent_lower = sent.lower()\n    if any(word in sent_lower for word in all_words_set):\n        matched_sentences.append({\"sentence\": sent})\n\ndf_matched = pd.DataFrame(matched_sentences)\ndf_matched.to_csv(matched_csv, sep=\";\", index=False, encoding=\"utf-8-sig\")\nprint(f\"{matched_csv} saved ({len(df_matched)} sentences matched)\")\n\n# ========================\n# Step 6: Fine-tune PLM\n# ========================\ndf_labelled = pd.read_csv(labelled_path, sep=\";\")\ndf_labelled.columns = df_labelled.columns.str.strip()\nlabel_map = {'offensive': 1, 'not_offensive': 0}\ndf_labelled['label'] = df_labelled['offensive'].map(label_map)\ndf_labelled['input_text'] = df_labelled['sentence']\ndf_hatebert = df_labelled[['input_text', 'label']]\n\nmodel_name = \"Hate-speech-CNERG/bert-base-uncased-hatexplain\"\n\nn_splits = 5\nskf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(df_hatebert, df_hatebert['label'])):\n    print(f\"=== Fold {fold+1}/{n_splits} ===\")\n    train_df = df_hatebert.iloc[train_idx].reset_index(drop=True)\n    val_df = df_hatebert.iloc[val_idx].reset_index(drop=True)\n\n    train_dataset = Dataset.from_pandas(train_df)\n    val_dataset = Dataset.from_pandas(val_df)\n\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    config = AutoConfig.from_pretrained(model_name)\n    config.num_labels = 2\n    config.problem_type = \"single_label_classification\"\n    model = AutoModelForSequenceClassification.from_pretrained(model_name, config=config, ignore_mismatched_sizes=True)\n\n    def tokenize(batch):\n        return tokenizer(batch[\"input_text\"], truncation=True, padding=True)\n\n    train_dataset = train_dataset.map(tokenize, batched=True)\n    val_dataset = val_dataset.map(tokenize, batched=True)\n\n    columns_to_remove = [\"input_text\", \"__index_level_0__\"]\n\n    train_dataset = train_dataset.remove_columns([c for c in columns_to_remove if c in train_dataset.column_names])\n    val_dataset = val_dataset.remove_columns([c for c in columns_to_remove if c in val_dataset.column_names])\n\n    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n    training_args = TrainingArguments(\n        output_dir=os.path.join(finetuned_model_dir, f\"fold_{fold}\"),\n        eval_strategy=\"epoch\",  # вместо evaluation_strategy\n        per_device_train_batch_size=16,\n        per_device_eval_batch_size=16,\n        num_train_epochs=3,\n        logging_dir=\"./logs\",\n        logging_steps=10,\n        save_strategy=\"no\",\n        report_to=[]\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset,\n        tokenizer=tokenizer,\n        data_collator=data_collator\n    )\n\n    trainer.train()\n\n# ========================\n# Step 7: Predictions on matched sentences\n# ========================\nhatebert_pipeline = pipeline(\n    \"text-classification\",\n    model=model,\n    tokenizer=tokenizer,\n    device=device\n)\n\npredicted_labels = []\npredicted_scores = []\n\nfor i in range(0, len(df_matched), batch_size_pred):\n    batch_texts = df_matched[\"sentence\"].iloc[i:i+batch_size_pred].tolist()\n    batch_texts = [t[:512] for t in batch_texts]\n    batch_results = hatebert_pipeline(batch_texts, truncation=True, max_length=512)\n    for res in batch_results:\n        predicted_labels.append(res[\"label\"])\n        predicted_scores.append(res[\"score\"])\n\ndf_matched[\"hatebert_label\"] = predicted_labels\ndf_matched[\"hatebert_score\"] = predicted_scores\ndf_matched.to_csv(\"matched_sentences_hatebert_predicted.csv\", index=False, sep=\";\", encoding=\"utf-8-sig\")\nprint(\"matched_sentences_hatebert_predicted.csv saved\")\n\n# ========================\n# Plotting\n# ========================\nsns.countplot(x=\"hatebert_label\", data=df_matched)\nplt.title(\"Class distribution of predicted offensive sentences\")\nplt.show()\n\nplt.hist(df_matched[\"hatebert_score\"], bins=20, color='orange', edgecolor='black')\nplt.title(\"Histogram of predicted offensive probabilities\")\nplt.xlabel(\"Probability (offensive)\")\nplt.ylabel(\"Number of samples\")\nplt.grid(True)\nplt.show()","metadata":{"_uuid":"32769433-4264-4239-a97b-2257d8351abe","_cell_guid":"85ebfeff-166e-471d-b1eb-d2512ec779aa","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}